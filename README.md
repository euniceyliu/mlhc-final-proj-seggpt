# mlhc-final-proj-seggpt

## Project Overview

This research project focuses on enhancing the interpretability of disease predictions derived from satellite imagery by employing the [SegGPT model](https://arxiv.org/abs/2304.03284). The SegGPT is a generalist image segmentation model renowned for its capability to manage diverse elements found in satellite imagery such as buildings, vegetation, and water bodies. The model uses an innovative coloring method during training, where segmentation is based on random color mappings linked to contextual information, enabling dynamic handling of various segmentation challenges without specific model retraining. By utilizing in-context learning along with one- and two-shot learning frameworks, SegGPT minimizes the need for extensive annotated datasets. This approach enables the effective identification of environmental factors critical to disease transmission dynamics, crucial for improving satellite-based disease prediction. Our goal is to provide actionable insights for public health interventions, particularly in under-resourced regions.

## Data

This project utilizes an open-source dataset available through [PhysioNet](https://physionet.org/content/multimodal-satellite-data/1.0.0/10_municipalities/#files-panel) and [HuggingFace](https://huggingface.co/datasets/MITCriticalData/SAT1_dataset_5_top_cities). The dataset includes time-series Sentinel-2 satellite images from five Colombian municipalities with high Dengue incidence: Medellin, Ibague, Cali, Villavicencio, and Cucuta. These areas are monitored weekly from 2016 to 2018, capturing 12 spectral bands in TIFF format, with a focus on the RGB bands for our analysis. The images have been preprocessed to a uniform resolution of 10 meters per pixel, though cloud coverage occasionally affects the data quality. This timeframe specifically avoids the confounding effects of the COVID-19 pandemic on disease transmission dynamics.

## Repository Structure
- **ImagesToTest-gamma/**: Contains gamma-corrected images used for testing the segmentation performance of the SegGPT model. These images help assess how well the model handles variations in image brightness and contrast.
- **ImagesToTest-rgb/**: Comprises test images in RGB format, used to evaluate the color image segmentation capabilities of the SegGPT model.
- **One-Shot_Gamma/**: Contains output images generated by the SegGPT model trained with the one-shot learning approach on gamma-corrected images. These images showcase the model’s ability to learn and perform segmentation from a single example.
- **One-Shot_RGB/**: Holds output images generated by the SegGPT model using the one-shot learning method on RGB images. This directory demonstrates the model's effectiveness in segmenting RGB images with minimal training data.
- **Two-Shot/**: Features output images from the SegGPT model employing two-shot learning techniques. These images highlight the enhanced understanding and segmentation performance achieved when the model is provided with two examples.
- **code/**: Contains all the source codes and Jupyter notebooks essential for the project’s operation and evaluation:
  - `calculate_metrics.ipynb`: This notebook is dedicated to calculating the Dice coefficient, a statistical tool used to measure the similarity between the model's predicted segmentation outputs and the actual segments.
  - `extraction_channel.ipynb`: Provides code for channel extraction from images, a crucial preprocessing step for handling different image formats and enhancing model input quality.
  - `gamma_correction.ipynb`: Implements gamma correction techniques on images to adjust their luminance, enhancing the model's segmentation accuracy under different lighting conditions.
  - `pixel_threshold.ipynb`: Experiments with pixel intensity thresholds, a preprocessing step to help distinguish between foreground and background in the images.
  - `run_seggpt_model.ipynb`: The main notebook used to setup, load, and run the SegGPT model. 

## Usage
1) Get the test image files in the correct folders and naming conventions. Place the ground-truth labeled images folder within {One-Shot, Two-Shot}/{Urban, Forest, Clouds, Water} folder, depending on the chosen in-context method & segmentation class. 
2) Add custom masks to the **One-Shot** or **Two-Shot** folders. These masks are crucial for testing how the model performs with different segmentation inputs.
3) Use `run_seggpt_model.ipynb` to perform predictions.
4) Ensure the output files are placed in the correct directory (the same folder as the labeled images folder) with appropriate names. The script expects the filenames `{original_filename}.png` for One-Shot outputs, and `output_{original_file_name}.png` for Two-Shot outputs.
5) Run `calculate_metrics.ipynb` to evaluate the model's performance.

## Note
Segmentation performance highly varies depending on the mask used. Therefore, we encourage users, especially those working within the One-Shot and Two-Shot learning contexts, to experiment by developing their own masks to optimize performance and achieve the best results. 
